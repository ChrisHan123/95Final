{"cells":[{"cell_type":"markdown","metadata":{"id":"1GNrOr-WiKQW"},"source":["![alt text](https://drive.google.com/uc?export=view&id=1DXUVHxd4t15mfuqMgMCLnsP4jWVI5EWz)\n","\n","---\n","\n","<br>\n","© 2023 Copyright The University of New South Wales - CRICOS 00098G\n","\n","**Authors**: Oscar Perez-Concha.\n","\n","**Communications**: If you have any questions, please email Oscar at: o.perezconcha@unsw.edu.au"]},{"cell_type":"markdown","source":["# Final Assignment (FA)"],"metadata":{"id":"N6T0wwT_GozJ"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"R1skKevpG0jB"}},{"cell_type":"markdown","source":[],"metadata":{"id":"QmhnQ1KbGY0B"}},{"cell_type":"markdown","metadata":{"id":"fN4p9w5eiKQY"},"source":["#####################################################################################\n","\n","Double-click to write down your name and surname.\n","\n","**Name 1 and Surname 1: Ruochen(Chris) Han z5353440\n","\n","**Honour Pledge** <p>\n","    \n","Declaration: <p>\n","    \n","    \n","I declare that this assessment item is my own work, except where acknowledged, and has not been submitted for academic credit elsewhere or previously, or produced independently of this course (e.g. for a third party such as your place of employment) and acknowledge that the assessor of this item may, for the purpose of assessing this item:\n","\n","1. Reproduce this assessment item and provide a copy to another member of the University; and/or\n","\n","2. Communicate a copy of this assessment item to a plagiarism checking service (which may then retain a copy of the assessment item on its database for the purpose of future plagiarism checking).\n","\n","**By writing you name and surname above, you certify that you have read and agreed to the honour pledge.**\n","\n","#####################################################################################"]},{"cell_type":"markdown","source":["#####################################################################################\n","\n","Double-click to write down your name and surname.\n","\n","**Name 2 and Surname 2:**\n","\n","**Honour Pledge** <p>\n","    \n","    \n","Declaration: <p>\n","    \n","    \n","I declare that this assessment item is my own work, except where acknowledged, and has not been submitted for academic credit elsewhere or previously, or produced independently of this course (e.g. for a third party such as your place of employment) and acknowledge that the assessor of this item may, for the purpose of assessing this item:\n","\n","1. Reproduce this assessment item and provide a copy to another member of the University; and/or\n","\n","2. Communicate a copy of this assessment item to a plagiarism checking service (which may then retain a copy of the assessment item on its database for the purpose of future plagiarism checking).\n","\n","**By wrting you name and surname above, you certify that you have read and agreed to the honour pledge.**\n","\n","#####################################################################################"],"metadata":{"id":"7EORMEMxuhJS"}},{"cell_type":"markdown","source":["---\n","# 1.  Health Data Science Scenario\n","\n","## 1.1. Research Question\n","\n","Our hospital has been very proactive in analysing data from its Electronic Medical Records (EMR). Through this analysis, we've made some interesting discoveries.\n","\n","We identified that readmitted patients are not only those who were sicker during their first admission but also those with less support after discharge or those without medical follow-up post-discharge.\n","Readmitted patients experienced high levels of emotional stress.\n","Readmitted patients were at a significantly higher risk of acquiring new infections while in the hospital.\n","Readmissions are highly costly. Patients who were readmitted for the reasons mentioned above tended to be sicker than during their first admission, and their length of stay was significantly longer than during their first admission."],"metadata":{"id":"hvLpFjsB9PYz"}},{"cell_type":"markdown","source":["Our hospital has implemented a pilot program named \"Redesigning Discharge,\" which involves a \"specialised unit\" that coordinates patients' discharges. This pilot program has been running for a year.\n","\n","This \"specialised unit\" is composed of a team of doctors, nurses, physiotherapists, and occupational therapists.\n","\n","The program consists of two main innovations.\n","\n","1. In the first part of the program, the doctors conduct a new discharge assessment protocol using innovative evidence-based tools to help them decide whether patients are ready to return to their usual place of residence or if it would be more advisable to transfer them to an intermediate or short-term care facility. As such, this program is set to replace the previous protocol concerning where patients should be discharged.\n","\n","2. The second part of the program includes a group of nurses, physiotherapists, and occupational therapists who visit patients at home or in the intermediate facility after discharge. The frequency of these visits is determined by an assessment conducted prior to discharge, averaging about five visits per patient. Among other things, the team's tasks include ensuring proper wound healing, checking medication compliance, coordinating with a general practitioner or the medical team at the intermediate facility, and assessing the patient's ability to perform basic daily activities such as navigating their environment, using the toilet, and showering. Furthermore, the specialised unit maintains regular telephone contact with patients to check on their well-being. Patients are also encouraged to reach out to the unit if they need any assistance.\n","\n","This pilot program has been very successful and it has been welcomed by all the stakeholders. Among other things, the program has significantly reduced the number of hospital readmissions. Moreover, the operating costs for this specialised unit have proven to be considerably lower than the costs associated with readmissions"],"metadata":{"id":"zXDNhvVF9gAL"}},{"cell_type":"markdown","source":["In terms of budget:\n","\n","1. The average cost of a day in the hospital is 6,000 dollars; readmitted patients tend to stay for an average of 4 days.\n","\n","2. An intervention by the specialised unit costs on average 2,000 dollars, which includes the cost of the average number of visits per patient, being 5 visits.\n","\n","3. The cost of an intermediate facility is not considered, as it's covered by Private Health Insurance or other Government Schemes. Additionally, our program is effectively 'donating' a team of medical professionals to these facilities. Therefore, we do not need to account for the cost of these intermediate facilities if a patient is discharged there.\n","\n","Our hospital is now prepared to implement \"Redesigning Discharge\" and extend the specialized unit service to all patients at risk of readmission. The challenge we face, however, is that we cannot predict in advance which patients are at risk of readmission. As such, we are currently unable to identify the specific patients we should be targeting"],"metadata":{"id":"M1cJxrpO9sAH"}},{"cell_type":"markdown","source":["Our research aims to address the following questions:\n","\n","1. Can we develop a machine learning algorithm that can predict the probability of a patient's readmission within 30 days before their discharge?\n","\n","2. How can we effectively communicate the performance and benefits of this algorithm to the hospital managers, enabling them to make an informed decision about its implementation?\n","\n","The main challenge we face is not only the development of an effective predictive algorithm, but also explaining its complexities in a clear, digestible manner for non-technical decision-makers."],"metadata":{"id":"PlVCjmaWCpzF"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"k_dDoHDsl9X2"}},{"cell_type":"markdown","source":["## 1.2. Instructions\n","\n","1.  We aim to develop a predictive model to forecast hospital readmissions within 30 days of discharge.\n","\n","> Consider the next points:\n","\n","> The predictive model will be run prior to discharge, and before a decision has been made regarding the need for the specialized unit or the place of discharge.\n","\n","> This forecast will primarly aid doctors and other clinicians in making a final decision about whether a patient will need the \"specialised unit\" or not.\n","\n","> In addition, as part of the new protocol, this forecast will also help decide whether the patient should return to their usual place of residence or be transferred to an intermediate facility. This new protocol is set to replace the previous one concerning where patients should be discharged.\n","\n","> We plan to use historical data from the hospital for this purpose. Unfortunately, the historical data with which we have been provided doesn't include any record of the pilot study that ran for a year, which could have assisted in training our model.\n","\n","\n","2. Review the research questions and make the appropriate decisions for building this predictive model, in accordance with the research question text.\n","\n","\n","3.   Thoroughly review and study the data provided. Please pay close attention to the data dictionary (PDF inside the data folder) and revisit the plots and graphs created during weeks 1, 2, and 3.\n","\n","4. Given that the hospital has already provided the data, we'll bypass the following steps in the health data science workflow (refer to the image below): Step 3 and substeps 3a,3b,3c,3d,3e: Data Gathering.\n","\n","5. In Step 4, substep 4a (Data visualization), visualize the data in your drafts, but do not include the graphs in your final submission.\n","\n","6. The hospital aims to capture as many \"readmissions\" as possible (true positives), even if this moderately increases the number of false positives within reason (i.e., patients not at risk of readmission classified as such). However, be mindful that the hospital does not have unlimited resources. An excessive number of patients identified as at-risk could inflate the operational costs of the \"specialised unit\" beyond sustainable levels.\n","\n","7. The data scientists at the hospital have asked that you only consider the Logistic Regression and Random Forest machine learning techniques.\n","\n","8. Design various machine learning algorithms with different hyperparameters. Select one and provide a rationale for your choice.\n","\n","9. Use the classification_report and confusion matrix metrics for model evaluation. If you use other metrics such as the ROC curve, do not include them in your final submission.\n","\n","10. Justification for your decisions is crucial. Provide a one or two sentence explanation for each section preceding the relevant Python code block. You are encouraged to use \"Sanity Checks\" during the construction of the algorithms. Please do not include them in your final submission.\n","\n","11. Ensure clear labels for printed results and briefly detail the steps followed.\n","\n","12. Remember to comment and document your code thoroughly, as this will likely serve as a foundation for team-based algorithm development in the future.\n","\n","13. Format: Jupyter Notebook.\n","\n","14. Programming Language: Python.\n","\n","15. Submission: Upload the Jupyter Notebook to the designated section in OpenLearning and on your GitHub space. Submissions will close precisely at the deadline.\n","\n","16. Only submit the final version of your document.\n","\n","17. Include your first and last names in the document's title. For example: Final-Assignment-AnnXu-MichaelRoss.ipynb\n","\n","18. Each question is worth 1 mark, except question 8, which is worth 2 marks.\n","\n","19. Marks will be deducted for failure to follow the previous instructions.\n","\n","20. Refer to the provided [rubric](https://unsw-my.sharepoint.com/:w:/g/personal/z3368601_ad_unsw_edu_au/EYaMoVWZ20hFoj9R8hL2PIIB4-ACN6c4bf4uCI66gzpEqA?e=tUJZiB).\n","\n"],"metadata":{"id":"CI9JCYKb-6tz"}},{"cell_type":"markdown","source":["![alt text](https://drive.google.com/uc?export=view&id=105SGqeyo8RgLhSO8mN7ZE5OsG0YiLPKt)"],"metadata":{"id":"9wK1oQbhq5hX"}},{"cell_type":"markdown","source":["## 1.3. Late Submission Penalty\n","\n","UNSW has a [standard](https://www.unsw.edu.au/arts-design-architecture/student-life/resources-support/protocols-guidelines)  late submission penalty of:\n","\n","\n","*   5% per day,\n","*   for all assessments where a penalty applies,\n","*   capped at five days (120 hours) from the assessment deadline, after which a student cannot submit an assessment,\n","*   and no permitted variation.\n","\n","Students are expected to manage their time to meet deadlines and to request extensions as early as possible before the deadline."],"metadata":{"id":"Z9Yo-Q0QKhSW"}},{"cell_type":"markdown","source":["## 1.4. Marks and Feedback\n","\n","Per UNSW's policy, the marks and feedback of this assignment will be released after UNSW releases the final marks."],"metadata":{"id":"a_aTlwYkKynH"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"3eE8VXdpl_I2"}},{"cell_type":"markdown","source":["## 1.5. Provided Python Code"],"metadata":{"id":"2n7dR4_xLlk-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"skbhUSeQiKQa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"db787603-dfdb-43ab-ba4b-01d2b62013a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Installing:  {'grid', 'shap'}\n"]}],"source":["# check required libraries are installed if not calling system to install\n","import sys\n","import subprocess\n","import pkg_resources\n","\n","required = {'numpy', 'pandas', 'plotnine', 'matplotlib', 'seaborn',\n","            'grid', 'shap', 'scikit-learn'}\n","installed = {pkg.key for pkg in pkg_resources.working_set}\n","missing = required - installed\n","\n","if missing:\n","    print('Installing: ', missing)\n","    python = sys.executable\n","    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n","# delete unwanted variables\n","del required\n","del installed\n","del missing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8p62DC4iKQb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"00e5c358-0390-4188-f8a3-accbfa3810ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount Google Drive\n","# We do not need to run this cell if you are not running this notebook in Google Colab\n","\n","if 'google.colab' in str(get_ipython()):\n","    from google.colab import drive # import drive from Gogle colab\n","    root = '/content/drive'     # default location for the drive\n","    # print(root)                 # print content of ROOT (Optional)\n","    drive.mount(root)\n","else:\n","    print('Not running on CoLab')"]},{"cell_type":"markdown","source":["Change the project paths and the paths according to where you have placed your files:"],"metadata":{"id":"T70EJU8pAzeY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kRNGzaEjiKQc"},"outputs":[],"source":["from pathlib import Path\n","\n","if 'google.colab' in str(get_ipython()):\n","    # EDIT THE PROJECT PATH IF DIFFERENT WITH YOUR ONE\n","    project_path = Path(root) / 'MyDrive' / 'HDAT9500' / 'final-assignment'\n","\n","    # OPTIONAL - set working directory according to your google drive project path\n","    # import os\n","    # Change directory to the location defined in project_path\n","    # os.chdir(project_path)\n","else:\n","    project_path = Path()"]},{"cell_type":"markdown","source":["\n","\n","---\n","Function to plot the confusion matrix:\n"],"metadata":{"id":"58McxBL6Angl"}},{"cell_type":"code","source":["def plot_confusion_matrix(confusion_matrix):\n","  # visualise the confusion matrix\n","  labels = ['No', 'Yes']\n","  ax = plt.subplot()\n","  sns.heatmap(confusion_matrix, annot = True, fmt = '.0f', ax = ax, cmap = 'viridis')\n","\n","  # labels, titles and ticks\n","  ax.set_xlabel('Predicted labels')\n","  ax.set_ylabel('True labels')\n","  ax.set_title('Confusion Matrix')\n","  ax.xaxis.set_ticklabels(labels)\n","  ax.yaxis.set_ticklabels(labels)"],"metadata":{"id":"ptWoRBD1os1T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"ANok5FMeLs8Y"}},{"cell_type":"markdown","source":["---\n","# 2. Questions:"],"metadata":{"id":"rixnC6G4Lvtu"}},{"cell_type":"markdown","metadata":{"id":"IbpJocChpcTR"},"source":["## Question 1: Docstring\n","\n","Create a docstring that includes the following:\n","\n","1. Purpose: The top-level aim of our program. (Please limit this to 50 words).\n","2. Author(s)\n","3. Dates\n","4. List of Variables, Constants, and Functions: Describe how each one is used in the program. Ensure that you choose informative variable names and thoroughly document your program (both through docstrings and comments). When required, units should accompany the variables. The list of variables must be written in different sections: General, Question 1, Question 2, ..., Question 9. For the list of variables within each section, list the variables, constants, and functions in alphabetical order.\n","5. Method: The \"Method\" section in a docstring typically refers to the approach or algorithm used in the program or function. Here, you would describe the sequence of operations or steps your code follows to accomplish its goal.\n","\n","Structure your docstring into distinct sections, with one section per point listed above.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"q2zvhr_epcTS"},"source":["<b> 1. Purpose: The aim of our program:</b>\n","\n","The aim is to develop a predictive model for hospital readmissions within 30 days of discharge, aiding doctors in decision-making for specialized units or transfers to intermediate facilities. We'll use historical data and Logistic Regression & Random Forest for model development and evaluation."]},{"cell_type":"markdown","source":["<b> 2. Author(s):</b>\n","\n","Ruochen(Chris Han) z5353440\n","\n"],"metadata":{"id":"YriqPVQpS3Bp"}},{"cell_type":"markdown","source":["<b> 3. Dates:</b>\n","\n","29JUL2023"],"metadata":{"id":"9kuKTvPSC_lX"}},{"cell_type":"markdown","metadata":{"id":"-3wGayW7marI"},"source":["<b> 4. List of variables and constants in alphabetical order:</b>\n","\n","################################################################################\n","\n","\n","\n","---\n","\n","<b> General: </b>\n","\n","---\n","Variables:\n","* `best_model`: This variable stores the best model obtained from the grid search, which includes the optimal hyperparameters.\n","* `best_params`: This variable stores the best hyperparameters found by the grid search.\n","* `columns_to_standardize`: list that contains the names of the columns in the dataset that need to be standardized. Standardization scales the values of these columns to have a mean of 0 and a standard deviation of 1.\n","* `grid_search.best_score_`: This variable stores the best F1 score achieved during the grid search on the training data. It represents the performance of the best model.\n","* `param_grid`: This is a dictionary that contains the hyperparameters to be tuned for the logistic regression model.\n","* `pipeline`: A pipeline that defines the sequence of data preprocessing steps and the machine learning model to be used.\n","\n","Functions:\n","* `ColumnTransformer`: A class from scikit-learn that allows you to apply different preprocessing steps to different subsets of the features (columns) in the dataset\n","* `GridSearchCV()`: A function from scikit-learn that performs grid search cross-validation to find the best hyperparameters for the model.\n","* `StandardScaler()`: This is a transformer from scikit-learn that standardizes the input features by removing the mean and scaling to unit variance.\n","\n","---\n","<b> Question 2: </b>\n","\n","Variables:\n","* `hospital_data`: Data from the pickle file 'hospital_data_final_assignment.pickle'\n","* `pickle_path`: Path of the pickle file\n","* `X`: The variable set that hospital data without readmission information\n","* `y`: The label set of readmission information\n","\n","Constants:\n","* `project_path`: The path where the pickle file is located.\n","\n","Functions:\n","* `drop()`: A DataFrame method that removes specified columns from the DataFrame.\n","* `map()`: A Series method that applies a mapping dictionary to the elements of the Series.\n","* `pickle.load()`: Loads the dataset from the pickle file specified by pickle_path.\n","* `value_counts()`: A Series method that returns the counts of unique values in the Series.\n","---\n","\n","<b> Question 3: </b>\n","\n","* `X_train`: Training set of X\n","* `X_test`: Testing set of X\n","* `y_train`: Training label set of X_train\n","* `y_test`: Testing label set set of X_test\n","\n","\n","---\n","<b> Question 4: Training, and hyper-parameter tuning of the Logistic Regression model.</b>\n","\n","Variables:\n","* `grid_search`: A scikit-learn GridSearchCV object that performs an exhaustive search over the hyperparameter grid defined in param_grid.\n","\n","Functions:\n","* `LogisticRegression()`: This is the logistic regression model from scikit-learn that will be used for classification.\n","\n","---\n","\n","<b>Question 5: Evaluation of the Logistic Regression: </b>\n","* `test_predictions`: This variable stores the predictions made by the logistic regression model on the test set (X_test).\n","* `test_confusion_matrix`: This variable stores the confusion matrix computed based on the actual labels Y_test and the predictions test_predictions of the logistic regression model on the test set.\n","* `train_predictions`: This variable stores the predictions made by the logistic regression model on the training set (X_train).\n","* `train_confusion_matrix`: This variable stores the confusion matrix computed based on the actual labels Y_train and the predictions train_predictions of the logistic regression model on the training set.\n","\n","---\n","\n","<b>Question 6: Train and tune the hyperparameters of the of the Random Forest model. </b>\n","\n","* `grid_search_rf`: This variable creates a GridSearchCV object for the random forest model\n","* `RandomForestClassifier()` is a class in the scikit-learn library used to create a random forest classifier.\n","\n","---\n","<b>Question 7: Evaluate the Random Forest model on both the training and test sets. Use the plot_confusion_matrix(confusion_matrix) function provided above to display the results. </b>\n","\n","* `train_predictions_RF`: This variable stores the predicted class labels for the training set using the best random forest model\n","* `train_confusion_matrix_RF`: This variable stores the confusion matrix for the training set predictions of the random forest model\n","* `test_predictions_RF`: This variable stores the predicted class labels for the test set using the best random forest model\n","* `test_confusion_matrix_RF`: This variable stores the confusion matrix for the test set predictions of the random forest model.\n","\n","---\n","<b>Question 8: Based on the research questions and instructions, determine which model, if any, you would choose. Justify whether or not you would deploy this model. </b>\n","\n","NAP\n","\n","---\n","<b>Question 9: Implement SHAP for the final model and describe your observations. If no model was selected in question 8, provide a justification and choose one model specifically for this SHAP analysis. Conclude by commenting on the results. </b>\n","\n","* `shap.initjs()`: This function initializes the JavaScript visualization for SHAP plots in Jupyter Notebook.\n","* `shap_explainer`: It creates a SHAP explainer for the Random Forest model\n","* `shap_value`: This computes the SHAP values for the test data (X_test) using the previously created explainer\n","* `shap.force_plot`: This generates a force plot for the first sample in the test set.\n","* `shap.summary_plot()` function is used to create a summary plot that shows the feature importance for multiple samples in the training set\n","---\n","\n","\n","################################################################################"]},{"cell_type":"markdown","source":["<b> 5. Methods:</b> The \"Method\" section in a docstring typically refers to the approach or algorithm used in the program or function. Here, you would describe the sequence of operations or steps your code follows to accomplish its goal.\n","################################################################################\n","  * View data: Collect data, view data, understand the structure, characteristics, and missing values of data, and conduct preliminary EDA.\n","  * Data preprocessing: Delete unnecessary variables and standardize the processing of numerical data.\n","  * Data partitioning: Partitioning data into training and test sets for training models, tuning, and evaluation.\n","  * Establish a logistic regression model, find the best hyperparameters, and use the training set for training.\n","  * Use a confusion matrix to evaluate the logistic regression model on the training set and the test set.\n","  * Establish the random forest model, find the best hyperparameters, and use the training set for training.\n","  * Use the confusion matrix to evaluate the random forest model on the training set and the test set.\n","  * The random forest model is selected as the final model, and a SHAP force diagram is created to interpret the prediction results of the model in the test data set, and the prediction results are for the positive class (class label 1).\n","\n","\n","\n","\n","\n","################################################################################"],"metadata":{"id":"Z_v4BgyRUoE1"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"ew1S59Vj1l2m"}},{"cell_type":"markdown","source":["---\n","## Question 2: Read and check the pickle file provided. Prepare the data so it can be used by the algorithms that you are going to create.\n","---"],"metadata":{"id":"DmqPFyIbnJci"}},{"cell_type":"markdown","source":["<b> Rationale: What are you doing to solve this question? - 75 word count limit:</b>\n","\n","\n","################################################################################\n","\n","By looking at the data dictionary, we found that some columns were identifier classes that had no real meaning, such as patient_id and admission_id, so we chose to remove them to reduce the risk of overfitting. The dataset is separated into a variable set and a label set. The label set contains the labels 'readmission_num', and variables set contain all variables except 'readmission', 'readmission_num'.\n","################################################################################"],"metadata":{"id":"1AogtSxHxEW5"}},{"cell_type":"code","source":["from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.compose import ColumnTransformer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import f1_score\n","from sklearn.ensemble import RandomForestClassifier\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","import shap\n"],"metadata":{"id":"T4JH09meLmyJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"63468a5a-dc3e-4424-8703-a1bd6dd0b71b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"]}]},{"cell_type":"code","source":["# Python code here (15 lines limit, 1 cell limit)\n","\n","#1. import some packages\n","import pickle\n","import pandas as pd\n","\n","#2. import the dataset\n","pickle_path = Path(project_path)/'hospital_data_final_assignment.pickle'\n","\n","with open(pickle_path, 'rb') as data:\n"," hospital_data = pickle.load(data)\n","\n","#3. delete two columns: \"admission_id\" and \"patient_id\"\n","hospital_data=hospital_data.drop([\"admission_id\",\"patient_id\"],axis=1)\n","#hospital_data.head()\n","\n","#4. create a new numeric variable \"readmission_num\" based on the \"readmission\" column.\n","hospital_data['readmission_num'] = hospital_data['readmission'].map({'yes': 1, 'no': 0})\n","\n","#5. Drop the 'readmission' and 'readmission_num' in X\n","X = hospital_data.drop(['readmission', 'readmission_num'], axis = 1)\n","# assigning 'readmission_num' to the Y\n","Y = hospital_data['readmission_num']\n","\n","\n","# # Print the value counts for each column\n","# for column in hospital_data.columns:\n","#     print(f\"--- the distribution of {column} ---\")\n","#     print(hospital_data[column].value_counts())\n","#     print(\"\\n\")\n"],"metadata":{"id":"DJJoTVRw049e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"1gsmCMIJ9oQ5"}},{"cell_type":"markdown","source":["---\n","## Question 3: Divide the data into 80% for training and 20% for testing, using a random seed of 30. Set the other hyperparameters as you deem appropriate.\n","---\n"],"metadata":{"id":"PRlUipB2-nMX"}},{"cell_type":"markdown","source":["<b> Rationale: What are you doing to solve this question?- 75 word count limit:</b>\n","\n","\n","################################################################################\n","\n","The 'train_test_split' has been used to divide dataset into an 80% training set and a 20% testing set. The X variable contains input features, and Y contains the corresponding target labels. Setting random_state to 30 to ensure consistent data splitting. The resulting variables, X_train, X_test, Y_train, and Y_test, hold the respective data splits for training and evaluating the data model. The parameter stratify=Y is used to ensure that the class distribution of the target variable Y is consistent in both the training set and the test set.\n","\n","\n","################################################################################"],"metadata":{"id":"7HjTXSR6-4cR"}},{"cell_type":"code","source":["# Python code here (2 lines limit, and 1 cell limit)\n","from sklearn.model_selection import train_test_split\n","\n","# Divide the data into 80% for training and 20% for testing, using a random seed of 30 as requested\n","# \"stratify=y\" is used to ensure that the proportion of class labels in the train and test datasets is the same as the original dataset\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.8, test_size=0.2, random_state=30, stratify=Y)"],"metadata":{"id":"5LeXrTXc-4cR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"ZcRSCX5s1kPR"}},{"cell_type":"markdown","source":["---\n","## Question 4: Train and tune the hyperparameters of the Logistic Regression model.\n","\n","Hyper-parameters:\n","\n","- `C` values: 10, 100.\n","- `class_weight`: only two pairs. Choose one of these two combinations and explain why.\n","\n","    * A weight of (80% for class 1, 20% for class 0) and (70% for class 1, 30% for class 0)\n","    * A weight of (80% for class 0, 20% for class 1) and (70% for class 0, 30% for class 1).\n","\n","- `penalty` values: elasticnet, None\n","- 3-fold cross-validation for the grid search.\n","- `f1` as the score to choose the best model in the grid search.\n","- `n_jobs`=-1.\n","- do not include the heatmaps in the final submission.\n","- do not change these hyper-parameters.\n","- keep the remaining set of hyper-parameters in the default state.\n","---\n"],"metadata":{"id":"b3v2JXmp1o5W"}},{"cell_type":"markdown","source":["<b> Rationale: What are you doing to solve this question?-100 word count limit:</b>\n","\n","\n","################################################################################\n","\n","1. The dataset exhibits class imbalance, with more samples in class 0 than class 1 in outcome variable. To address this, we assign higher weight to class 1 in weight combinations. This balances the impact of each class during model training, prioritizing accurate identification of class 1 instances (readmission).\n","\n","2. The code preprocesses data by standardizing chosen columns and builds a logistic regression model with hyperparameter tuning through grid search. Evaluation is based on the F1 score using test data. The approach aims to improve class 1 prediction by emphasizing its significance in the training process.\n","\n","\n","\n","################################################################################"],"metadata":{"id":"p845NQAlAY-U"}},{"cell_type":"code","source":["# Python code here (15 lines limit, and 2 cells limit)\n","\n","# Define columns that need to be normalized\n","columns_to_standardize = ['los', 'Age', 'number_diagnoses', 'num_lab_procedures', 'num_procedures', 'num_medications']\n","\n","# Create a Pipeline that includes data standardization and logistic regression models\n","pipeline = Pipeline([\n","    ('preprocessor', ColumnTransformer(transformers=[('num', StandardScaler(), columns_to_standardize)])),\n","    ('logreg', LogisticRegression())\n","])\n","\n","# Define hyperparameters to tune\n","param_grid = {\n","    'logreg__C': [10, 100],\n","    'logreg__class_weight': [{0: 0.2, 1: 0.8}, {0: 0.3, 1: 0.7}],\n","    'logreg__penalty': ['elasticnet', None],\n","    'logreg__solver': ['saga']\n","}\n","\n","# Create GridSearchCV object with logistic regression model and hyperparameters with F1 as the scoring metric and 3-fold cross-validation, and all available CPU cores will be used to perform tasks in parallel.\n","grid_search = GridSearchCV(pipeline, param_grid, scoring='f1', cv=3, n_jobs=-1)\n","\n","# Fit the GridSearchCV on training data\n","grid_search.fit(X_train, Y_train)\n","\n","# Get the best hyperparameters and the best model\n","best_params = grid_search.best_params_\n","best_model = grid_search.best_estimator_\n","\n","# Print F1 score\n","print(\"Best F1 score:\", grid_search.best_score_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80niIb15BFB0","outputId":"b14ddc09-1afd-4d7f-ca0d-aaa0c562465d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n","12 fits failed out of a total of 24.\n","The score on these train-test partitions for these parameters will be set to nan.\n","If these failures are not expected, you can try to debug them by setting error_score='raise'.\n","\n","Below are more details about the failures:\n","--------------------------------------------------------------------------------\n","12 fits failed with the following error:\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n","    estimator.fit(X_train, y_train, **fit_params)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n","    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1291, in fit\n","    fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\", line 63, in __call__\n","    return super().__call__(iterable_with_config)\n","  File \"/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\", line 1855, in __call__\n","    return output if self.return_generator else list(output)\n","  File \"/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\", line 1784, in _get_sequential_output\n","    res = func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\", line 123, in __call__\n","    return self.function(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 521, in _logistic_regression_path\n","    alpha = (1.0 / C) * (1 - l1_ratio)\n","TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n","\n","  warnings.warn(some_fits_failed_message, FitFailedWarning)\n","/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.89070914        nan 0.89731234        nan 0.89070914\n","        nan 0.89725793]\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Best F1 score: 0.8973123433457797\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","The code is partially working, but some fits failed due to the incompatibility of the 'elasticnet' penalty with the 'lbfgs' solver used in the logistic regression model.\n"],"metadata":{"id":"98-HHV0DNkkC"}},{"cell_type":"markdown","source":["---\n","## Question 5: Evaluate the Logistic Regression model on both the training and test sets. Use the `plot_confusion_matrix(confusion_matrix)` function provided above to display the results.\n","---"],"metadata":{"id":"y2BLfWf9NmWf"}},{"cell_type":"markdown","source":["<b> Rationale: Why are we evaluating the model in the training set? Why are we evaluating the model in the test set? What are you doing to solve this question?- 120 word count limit:</b>\n","\n","\n","################################################################################\n","\n","1. The evaluation results on the training set can be used to evaluate the goodness of training fit of the model. The results of the evaluation on the test set can judge the model's performance in the real world, that is, whether it can accurately generalize to new data, which is used to measure the model's predictive accuracy and effectiveness.\n","\n","2. This code evaluates the Logistic Regression model's performance on the training and test sets and plots the confusion matrices. The confusion matrices show the true positive, false positive, true negative, and false negative counts for each class. It helps assess the model's accuracy and identify classification errors.\n","\n","################################################################################"],"metadata":{"id":"T0ZoopThNyaW"}},{"cell_type":"code","source":["# Python code here (10 lines limit, and 2 cells limit)\n","\n","# Evaluate the Logistic Regression model on the training set\n","train_predictions = best_model.predict(X_train)\n","train_confusion_matrix = confusion_matrix(Y_train, train_predictions)\n","\n","# print the confusion matrices for LR training set and not showing heat map as required\n","# plot_confusion_matrix(train_confusion_matrix)\n","print(train_confusion_matrix)\n","print(classification_report(Y_train, train_predictions))\n","\n","# print F1 for Logistic Regression model on the training set\n","print(\"F1 score LR training =\", f1_score(Y_train, train_predictions))\n"],"metadata":{"id":"L1GEJ8kGNyaW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f4dbccfa-4177-4abe-b4c4-af2fc619ee75"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[44285  1593]\n"," [  475  9060]]\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.97      0.98     45878\n","           1       0.85      0.95      0.90      9535\n","\n","    accuracy                           0.96     55413\n","   macro avg       0.92      0.96      0.94     55413\n","weighted avg       0.97      0.96      0.96     55413\n","\n","F1 score LR training = 0.8975629086586091\n"]}]},{"cell_type":"code","source":["# Evaluate the Logistic Regression model on the test set\n","test_predictions = best_model.predict(X_test)\n","test_confusion_matrix = confusion_matrix(Y_test, test_predictions)\n","\n","# print the confusion matrices for LR test set and not showing heat map as required\n","# plot_confusion_matrix(test_confusion_matrix)\n","print(test_confusion_matrix)\n","print(classification_report(Y_test, test_predictions))\n","\n","# print F1 for Logistic Regression model on the test set\n","print(\"F1 score LR test =\", f1_score(Y_test, test_predictions))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4jYZlLByfME","outputId":"e3d3db3d-5e1c-4ff4-eafe-98fd4e23538e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[11053   417]\n"," [  125  2259]]\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.96      0.98     11470\n","           1       0.84      0.95      0.89      2384\n","\n","    accuracy                           0.96     13854\n","   macro avg       0.92      0.96      0.93     13854\n","weighted avg       0.96      0.96      0.96     13854\n","\n","F1 score LR test = 0.8928853754940711\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"CTIi2cd1yugI"}},{"cell_type":"markdown","source":["---\n","## Question 6: Train and tune the hyperparameters of the of the Random Forest model.  \n","\n","Fixed hyper-parameters (do not change the values of these hyper-parameters):\n","\n","- `n_estimators`: 125, 150\n","- `class_weight`: Only two pairs. Choose one of these two combinations (either combination 1 or combination 2) and explain why.\n","1. A weight of (80% for class 1, 20% for class 0) and (70% for class 1, 30% for class 0)\n","2.  A weight of (80% for class 0, 20% for class 1) and (70% for class 0, 30% for class 1).\n","- 3-fold cross-validation for the grid search.\n","- Select the score to choose the best model in the grid search.\n","- `n_jobs`=-1\n","- do not include the heatmaps in the final submission\n","\n","Other hyper-parameters:\n","\n","- `max_features`: 20, 30\n","- `min_samples_split`: 20, 25\n","- you can change the previous hyper-parameters (`max_features` and `min_samples_split`) or add other hyper-parameters if you wish. An explanation must be given to why you made that decision.\n","\n","---\n"],"metadata":{"id":"iTQtpo_8yvX3"}},{"cell_type":"markdown","source":["<b> Rationale: What are you doing to solve this question?-150 word count limit:</b>\n","\n","\n","################################################################################\n","\n","The dataset exhibits class imbalance, with more samples in class 0 than class 1 in outcome variable. To address this, we assign higher weight to class 1 in weight combinations. This balances the impact of each class during model training, prioritizing accurate identification of class 1 instances (readmission).\n","\n"," We create a pipeline with data standardization and RandomForestClassifier. Defining a different set of hyperparameters, such as 'n_estimators', 'class_weight', 'max_features', and 'min_samples_split', and perform a GridSearchCV with 3-fold cross-validation using the F1 score as the scoring metric.\n","\n","\n","################################################################################"],"metadata":{"id":"fA5-7Qpe2SPC"}},{"cell_type":"code","source":["# Python code here (15 lines limit, and 2 cells limit)\n","\n","# Define columns that need to be normalized\n","columns_to_standardize = ['los', 'Age', 'number_diagnoses', 'num_lab_procedures', 'num_procedures', 'num_medications']\n","\n","# Create a Pipeline that includes data standardization and logistic regression models\n","pipeline = Pipeline([\n","    ('preprocessor', ColumnTransformer(transformers=[('num', StandardScaler(), columns_to_standardize)])),\n","    ('rf', RandomForestClassifier())\n","])\n","\n","# Define hyperparameters to tune\n","param_grid = {\n","   'rf__n_estimators': [125, 150],\n","    'rf__class_weight': [{0: 0.2, 1: 0.8}, {0: 0.3, 1: 0.7}],\n","    'rf__max_features': [20, 30],\n","    'rf__min_samples_split': [20, 25]\n","}\n","\n","# Create GridSearchCV object with logistic regression model and hyperparameters\n","grid_search_rf = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1,scoring='f1')\n","\n","# Fit the GridSearchCV on training data\n","grid_search_rf.fit(X_train, Y_train)\n","\n","# Get the best hyperparameters and the best model\n","best_params_rf = grid_search_rf.best_params_\n","best_model_rf = grid_search_rf.best_estimator_"],"metadata":{"id":"o3O97zICHeYM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"_zu7iaT8AANM"}},{"cell_type":"markdown","source":["---\n","## Question 7: Evaluate the Random Forest model on both the training and test sets. Use the `plot_confusion_matrix(confusion_matrix)` function provided above to display the results.\n","---"],"metadata":{"id":"xwh91teY3VgL"}},{"cell_type":"markdown","source":["<b> Rationale: Why are we evaluating the model in the training set? Why are we evaluating the model in the test set? What are you doing to solve this question?- 120 word count limit:</b>\n","\n","################################################################################\n","1. The evaluation results on the training set can be used to evaluate the goodness of training fit of the model. The results of the evaluation on the test set can judge the model's performance in the real world, that is, whether it can accurately generalize to new data, which is used to measure the model's predictive accuracy and effectiveness.\n","\n","2. This code evaluates the Random Forest model's performance on the training and test sets and plots the confusion matrices. The confusion matrices show the true positive, false positive, true negative, and false negative counts for each class. It helps assess the model's accuracy and identify classification errors.\n","\n","\n","################################################################################"],"metadata":{"id":"hiHbV8Sf3VgT"}},{"cell_type":"code","source":["# Python code here (10 lines limit, and 2 cells limit)\n","\n","\n","# Evaluate the Random Forest model on the training set\n","train_predictions_RF = best_model_rf.predict(X_train)\n","train_confusion_matrix_RF = confusion_matrix(Y_train, train_predictions_RF)\n","\n","# print the confusion matrices for RF training set and not showing heat map as required\n","# plot_confusion_matrix(train_confusion_matrix_RF)\n","print(train_confusion_matrix_RF)\n","print(classification_report(Y_train, train_predictions_RF))\n","\n","# print F1 for Logistic Regression model on the training set\n","print(\"F1 score RF training =\", f1_score(Y_train, train_predictions_RF))\n"],"metadata":{"id":"Yk4wPAlV3VgU","colab":{"base_uri":"https://localhost:8080/"},"outputId":"de382e46-5a01-4eed-9270-fa009dd16758"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[44732  1146]\n"," [  160  9375]]\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.98      0.99     45878\n","           1       0.89      0.98      0.93      9535\n","\n","    accuracy                           0.98     55413\n","   macro avg       0.94      0.98      0.96     55413\n","weighted avg       0.98      0.98      0.98     55413\n","\n","F1 score RF training = 0.9348823294774631\n"]}]},{"cell_type":"code","source":["# Evaluate the Random Forest model on the testing set\n","test_predictions_RF = best_model_rf.predict(X_test)\n","test_confusion_matrix_RF = confusion_matrix(Y_test, test_predictions_RF)\n","\n","# print the confusion matrices for RF testing set and not showing heat map as required\n","# plot_confusion_matrix(test_confusion_matrix_RF)\n","print(test_confusion_matrix_RF)\n","print(classification_report(Y_test, test_predictions_RF))\n","\n","# print F1 for Logistic Regression model on the test set\n","print(\"F1 score RF test =\", f1_score(Y_test, test_predictions_RF))\n"],"metadata":{"id":"ZZedf1x952oa","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5be255ca-42b1-400d-88c8-874560495583"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[11104   366]\n"," [  101  2283]]\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.97      0.98     11470\n","           1       0.86      0.96      0.91      2384\n","\n","    accuracy                           0.97     13854\n","   macro avg       0.93      0.96      0.94     13854\n","weighted avg       0.97      0.97      0.97     13854\n","\n","F1 score RF test = 0.9072123981720644\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"4CsbaxoQ3VgU"}},{"cell_type":"markdown","source":["---\n","## Question 8: Based on the research questions and instructions, determine which model, if any, you would choose. Justify whether or not you would deploy this model.\n","---"],"metadata":{"id":"eMBZph5HA6AN"}},{"cell_type":"markdown","source":["<b> Rationale: 350 word count limit </b>\n","\n","################################################################################\n","\n","The random forest model works better.\n","\n","1. Random Forest Model（1 is the positive class）:\n","\n","    F1 score of testing set: 0.91\n","    \n","    F1 score of training set: 0.93\n","\n","2. Logistic Regression Model（1 is the positive class）:\n","\n","    F1 score of testing set: 0.89\n","    \n","    F1 score of training set: 0.90\n","\n","\n","\n","\n","\n","3. Judging from F1 scores, recall and precision :\n","\n","    The random forest model performed best on the test set, with a relatively high F1 score（0.91）and indicating good generalization to new data.\n","    The F1 score of the training set (0.93) is also very high, indicating that it has good fitting ability on the training data, however, there is a slight overfitting concern due to the performance difference between the training and testing sets.\n","\n","    The logistic regression model has a lower F1 score (0.89) on the test set and also has a slightly lower performance on the training set(0.90) than the random forest model.\n","\n","    In both the test set and the training set, the recall and precision values of the random forest model are higher than those of the logistic regression model.\n","\n","    Considering the performance on the test set and the overfitting on the training set, we can draw the following conclusions:\n","\n","    （1）The random forest model has a higher F1 score and better performance on the test set and the training set.\n","\n","    （2）Depending on the performance of the test set, we can choose the random forest model as the final choice as it performed slightly better based on the F1 score and general performance\n","\n","    （3）Since the random forest model can identify potential readmissions relatively accurately (recall value is larger), patients at risk of readmissions can save hospitalization costs (because one day of hospitalization costs 6000 and takes 4 days).\n","\n","\n","################################################################################"],"metadata":{"id":"AXP5G5nrA6AO"}},{"cell_type":"code","source":["# Python code here (10 lines limit, and 2 cells limit)\n","from sklearn.metrics import classification_report\n","\n","# Predict results on test data\n","test_predictions = best_model.predict(X_test)\n","\n","# Print reports of recall, accuracy and F1 scores (for test data)\n","print(\"LR Testing Set Performance:\")\n","print(classification_report(Y_test, test_predictions))"],"metadata":{"id":"heyMLNJtA6AO","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f3641cd1-ea52-4fa2-8eba-051ab6a1dfe0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["LR Testing Set Performance:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.96      0.98     11470\n","           1       0.84      0.95      0.89      2384\n","\n","    accuracy                           0.96     13854\n","   macro avg       0.92      0.96      0.93     13854\n","weighted avg       0.96      0.96      0.96     13854\n","\n"]}]},{"cell_type":"code","source":["# Predict results on test data\n","test_predictions_RF = best_model_rf.predict(X_test)\n","\n","# Print reports of recall, accuracy and F1 scores (for test data)\n","print(\"RF Testing Set Performance:\")\n","print(classification_report(Y_test, test_predictions_RF))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IdSJN-MHPi3z","outputId":"22295838-c311-4081-bbd5-6198dc1e3a35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["RF Testing Set Performance:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.97      0.98     11470\n","           1       0.86      0.96      0.91      2384\n","\n","    accuracy                           0.97     13854\n","   macro avg       0.93      0.96      0.94     13854\n","weighted avg       0.97      0.97      0.97     13854\n","\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"l63_Jk0mA6AP"}},{"cell_type":"markdown","source":["---\n","## Question 9:  Implement SHAP for the final model and describe your observations. If no model was selected in question 8, provide a justification and choose one model specifically for this SHAP analysis. Conclude by commenting on the results.\n","---"],"metadata":{"id":"u0Wb9iHWASdT"}},{"cell_type":"markdown","source":["<b> Explanation of what you observe - 250 word count limit:</b>\n","\n","################################################################################\n","\n","'shap.force_plot' is used to explain predictions for individual samples. For example, for the given patient with specific feature values (age, number_diagnoses, los, and num_medications), the prediction value is 0.52, which is higher than the base value and gets transformed to a class \"1\" prediction. Features displayed in red color have a positive influence on the prediction value, while features displayed in blue color have a negative influence.\n","\n","On the other hand, 'shap.summary_plot' provides a comprehensive display of feature importance for multiple samples. In the plot, the 'los' feature has the largest bar, indicating that it has the most significant impact on the predictions. Following that, 'num_medications', 'num_medications', and 'num_lab_procedure' also have notable contributions to the random forest predictions. 'Age' is the least impactful variable among the five features, while the other variables seem to have negligible impact on the predictions.\n","\n","The results make sense, as longer hospital stays, more medications, more diagnostics, and more lab tests, as well as an increased age, are factors that can lead to a higher chance of readmission.\n","\n","Using both 'shap.force_plot' and 'shap.summary_plot' together can provide a comprehensive understanding of the random forest model's predictions and their explanations. This combined approach allows for a more in-depth analysis of the model's behavior and the importance of different features in making predictions.\n","\n","\n","\n","################################################################################"],"metadata":{"id":"1IlYvZ83ASda"}},{"cell_type":"code","source":["# Python code here (10 lines limit, and 2 cells limit)\n","\n","# Initializing\n","shap.initjs()\n","\n","# Shap explainer and values creating\n","shap_explainer = shap.TreeExplainer(grid_search_rf.best_estimator_['rf'])\n","shap_value = shap_explainer.shap_values(X_test)\n","\n","# SHAP force plot\n","shap.force_plot(shap_explainer.expected_value[1], shap_value[1][0,:], X_test.iloc[0,:], matplotlib=True)"],"metadata":{"id":"wh2VJms9ASda","colab":{"base_uri":"https://localhost:8080/","height":249},"outputId":"08a7a75c-2b08-4326-eba8-9c6083d47d6e","executionInfo":{"status":"error","timestamp":1690673266254,"user_tz":-600,"elapsed":8,"user":{"displayName":"刘琪峰","userId":"16286446596774163081"}}},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-bd5c40181e52>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initializing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitjs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Shap explainer and values creating\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'shap' is not defined"]}]},{"cell_type":"code","source":["shap.summary_plot(shap_value, X_train)"],"metadata":{"id":"c5a4atLRqR3y","executionInfo":{"status":"aborted","timestamp":1690673266255,"user_tz":-600,"elapsed":4,"user":{"displayName":"刘琪峰","userId":"16286446596774163081"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"hrD9unXPAZQL"}},{"cell_type":"markdown","metadata":{"id":"TYOVWp4ViKQq"},"source":["© 2023 Copyright The University of New South Wales - CRICOS 00098G"]}],"metadata":{"colab":{"provenance":[]},"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"},"kernelspec":{"display_name":"Python 3.7.4 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}